{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "import findspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://medium.com/@kashif.sohail/how-to-read-compressed-csv-files-from-s3-using-local-pyspark-and-jupyter-notebook-b30e50c41b95\n",
    "- http://bartek-blog.github.io/python/spark/2019/04/22/how-to-access-s3-from-pyspark.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('/Users/paulogier/81-GithubPackages/Udacity-Data-Engineer-NanoDegree/P4-Data_Lake_with_Spark/p4src/etl/dbuser_config.cfg')\n",
    "os.environ['AWS_KEY_ID'] = config.get(\"AWS\", \"KEY\")\n",
    "os.environ['AWS_SECRET'] = config.get(\"AWS\", 'SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('S3CSVRead').getOrCreate()\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.awsAccessKeyId\", os.environ.get('AWS_KEY_ID'))\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.awsSecretAccessKey\", os.environ.get('AWS_SECRET'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female| 35|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male| 35|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mycsv = \"s3a://dendpaulogieruswest2/sampledata/titanic-data.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").csv(mycsv)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/35409539/corrupt-record-error-when-reading-a-json-file-into-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "+--------------------+----------+------+---+-------------+----------+\n",
      "|               email|first_name|gender| id|   ip_address| last_name|\n",
      "+--------------------+----------+------+---+-------------+----------+\n",
      "|rjosskoviz0@forbe...|   Randolf|  Male|  1| 51.159.229.1| Josskoviz|\n",
      "|coloughlin1@huged...|    Camala|Female|  2| 145.22.1.196|O'Loughlin|\n",
      "|abofield2@statcou...|      Arie|  Male|  3| 56.82.90.115|   Bofield|\n",
      "|mhenkens3@utexas.edu|      Mark|  Male|  4| 88.25.101.43|   Henkens|\n",
      "|atilliard4@nature...|    Amelia|Female|  5|23.17.182.175|  Tilliard|\n",
      "+--------------------+----------+------+---+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myprefix = \"s3a://dendpaulogieruswest2/sampledata/connectionssample*\"\n",
    "df = spark.read.options(multiLine=True).json(myprefix)\n",
    "print(df.count())\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "+--------------------+----------+------+---+-------------+----------+\n",
      "|               email|first_name|gender| id|   ip_address| last_name|\n",
      "+--------------------+----------+------+---+-------------+----------+\n",
      "|rjosskoviz0@forbe...|   Randolf|  Male|  1| 51.159.229.1| Josskoviz|\n",
      "|coloughlin1@huged...|    Camala|Female|  2| 145.22.1.196|O'Loughlin|\n",
      "|abofield2@statcou...|      Arie|  Male|  3| 56.82.90.115|   Bofield|\n",
      "|mhenkens3@utexas.edu|      Mark|  Male|  4| 88.25.101.43|   Henkens|\n",
      "|atilliard4@nature...|    Amelia|Female|  5|23.17.182.175|  Tilliard|\n",
      "+--------------------+----------+------+---+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import types as T\n",
    "s = T.StructType([\n",
    "    T.StructField(\"email\", T.StringType()),\n",
    "    T.StructField(\"first_name\", T.StringType()),\n",
    "    T.StructField(\"gender\", T.StringType()),\n",
    "    T.StructField(\"id\", T.IntegerType()),\n",
    "    T.StructField(\"ip_address\", T.StringType()),\n",
    "    T.StructField(\"last_name\", T.StringType())\n",
    "])\n",
    "df = spark.read.options(multiLine=True).json(myprefix, schema=s)\n",
    "print(df.count())\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
