{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('/Users/paulogier/81-GithubPackages/Udacity-Data-Engineer-NanoDegree/P4-Data_Lake_with_Spark/p4src/etl/dbuser_config.cfg')\n",
    "os.environ['AWS_KEY_ID'] = config.get(\"AWS\", \"KEY\")\n",
    "os.environ['AWS_SECRET'] = config.get(\"AWS\", 'SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('S3CSVRead').getOrCreate()\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.awsAccessKeyId\", os.environ.get('AWS_KEY_ID'))\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.awsSecretAccessKey\", os.environ.get('AWS_SECRET'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading log (event) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "s = T.StructType([\n",
    "    T.StructField(\"artist\", T.StringType()),\n",
    "    T.StructField(\"auth\", T.StringType()),\n",
    "    T.StructField(\"firstName\", T.StringType()),\n",
    "    T.StructField(\"gender\", T.StringType()),\n",
    "    T.StructField(\"itemInSession\", T.IntegerType()),\n",
    "    T.StructField(\"lastName\", T.IntegerType()),\n",
    "    T.StructField(\"length\", T.DoubleType()),\n",
    "    T.StructField(\"level\", T.StringType()),\n",
    "    T.StructField(\"location\", T.StringType()),\n",
    "    T.StructField(\"method\", T.StringType()),\n",
    "    T.StructField(\"page\", T.StringType()),\n",
    "    T.StructField(\"registration\", T.IntegerType()),\n",
    "    T.StructField(\"sessionId\", T.IntegerType()),\n",
    "    T.StructField(\"song\", T.StringType()),\n",
    "    T.StructField(\"status\", T.IntegerType()),\n",
    "    T.StructField(\"ts\", T.TimestampType()),\n",
    "    T.StructField(\"userAgent\", T.StringType()),\n",
    "    T.StructField(\"userId\", T.IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://udacity-dend/log_data/*/*/*.json\n",
      "8056\n"
     ]
    }
   ],
   "source": [
    "mybucket= 'udacity-dend'\n",
    "myprefix = 'log_data/'\n",
    "mypath = \"s3a://\"+mybucket+'/' + myprefix + '*/*/*.json'\n",
    "print(mypath)\n",
    "df = spark.read.json(mypath, schema=s)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = T.StructType([\n",
    "        T.StructField(\"artist_id\", T.StringType()),\n",
    "        T.StructField(\"artist_latitude\", T.DoubleType()),\n",
    "        T.StructField(\"artist_location\", T.StringType()),\n",
    "        T.StructField(\"artist_longitude\", T.DoubleType()),\n",
    "        T.StructField(\"artist_name\", T.StringType()),\n",
    "        T.StructField(\"duration\", T.DoubleType()),\n",
    "        T.StructField(\"num_songs\", T.IntegerType()),\n",
    "        T.StructField(\"title\", T.StringType()),\n",
    "        T.StructField(\"year\", T.IntegerType()),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3c = boto3.client(\"s3\", \n",
    "                  region_name='us-west-2', \n",
    "                  aws_access_key_id=os.environ.get('AWS_KEY_ID'), \n",
    "                  aws_secret_access_key=os.environ.get('AWS_SECRET')\n",
    "                 )\n",
    "m = []\n",
    "for key in s3c.list_objects(Bucket='udacity-dend', Prefix='song_data/A/A/')['Contents']:\n",
    "    k = key['Key']\n",
    "    m.append(k)\n",
    "print(len(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604\n"
     ]
    }
   ],
   "source": [
    "mypath2= \"s3a://udacity-dend/song_data/A/A/*/*.json\"\n",
    "df2 = spark.read.json(mypath2, schema=s2)\n",
    "print(df2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
